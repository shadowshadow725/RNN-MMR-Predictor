{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_mmr(elos: Dict) -> int:\n",
    "    s, d = 0, 0\n",
    "\n",
    "    for j in elos['elo']:\n",
    "\n",
    "        if j:\n",
    "            s += j\n",
    "            d += 1\n",
    "    return s/d\n",
    "\n",
    "\n",
    "def read_data_to_xs_ys():\n",
    "    matches = os.listdir(os.getcwd()+'\\\\DataCollection\\\\data\\\\')\n",
    "    ys = []\n",
    "    xs = []\n",
    "\n",
    "    for i in matches:\n",
    "        try:\n",
    "            if 'zip' not in i:\n",
    "                f = open(os.getcwd()+'\\\\DataCollection\\\\data\\\\' + i)\n",
    "                data = json.load(f)\n",
    "                # return data['timeline'], data['elo']\n",
    "\n",
    "                if data['timeline'] and len(data['timeline'])> 14:\n",
    "                    if data['elo']:\n",
    "                        \n",
    "                        rating_class = rank_classification(average_mmr(data))\n",
    "                        if rating_class:\n",
    "                            xs.append(data['timeline'][:15])\n",
    "                            ys.append(rating_class)\n",
    "        except:\n",
    "            print(i, 'failed')\n",
    "\n",
    "    return xs, ys\n",
    "\n",
    "def rank_classification(rating: float):\n",
    "    mmr_ranges = [\n",
    "        [0, 112],\n",
    "        [112, 227], \n",
    "        [227, 479],\n",
    "        [497, 579],\n",
    "        [579, 757], # bronze 4 \n",
    "        [757, 963], \n",
    "        [963, 1094], \n",
    "        [1094, 1207], \n",
    "        [1207, 1308],  # silver 4 \n",
    "        [1308, 1418], \n",
    "        [1418, 1528], \n",
    "        [1528, 1619],\n",
    "        [1619, 1702], # gold 4 \n",
    "        [1702, 1793],\n",
    "        [1793, 1896], \n",
    "        [1896, 1980], \n",
    "        [1980, 2045], # plat 4 \n",
    "        [2045, 2146], \n",
    "        [2146, 2255], \n",
    "        [2255, 2329], \n",
    "        [2329, 2396], # diamond 4 \n",
    "        [2396, 2487], \n",
    "        [2487, 2602], \n",
    "        [2602, 2729], \n",
    "        [2729, 2893], # masters \n",
    "        [2893, 3126], # grandmaster \n",
    "        [3126, 3386] # challengers \n",
    "        \n",
    "    ]\n",
    "    for i in range(len(mmr_ranges)):\n",
    "        if mmr_ranges[i][0] <= rating < mmr_ranges[i][1]:\n",
    "            return i\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints failed\n"
     ]
    }
   ],
   "source": [
    "xs, ys = read_data_to_xs_ys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714 714\n",
      "torch.Size([15, 50])\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(xs), len(ys))\n",
    "# torch.cuda.is_available()\n",
    "# print(torch.version.cuda)\n",
    "\n",
    "# train = [xs[:600], ys[:600]]\n",
    "# valid = [xs[600:], ys[600:]]\n",
    "tmp = [] \n",
    "for i in range(len(xs)):\n",
    "    tmp.append((torch.tensor(xs[i]), ys[i]))\n",
    "train = tmp[:600]\n",
    "valid = tmp[600:]\n",
    "\n",
    "print(train[0][0].size())\n",
    "print(train[0][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  554,   581,   500,     0,     0,   557,   345,   500,     0,\n",
       "            0,   335,   269,   500,     0,     0,   194,   457,   500,\n",
       "            0,     0,   329,   650,   500,     0,     0, 14180, 14271,\n",
       "          500,     0,     0, 14176, 14506,   500,     0,     0, 14398,\n",
       "        14582,   500,     0,     0, 14539, 14394,   500,     0,     0,\n",
       "        14404, 14201,   500,     0,     0],\n",
       "       [ 1292, 12240,   500,     0,     0,  6937,  2946,   500,     0,\n",
       "            0,  8816,  4364,   500,     0,     0, 10490,  3040,   500,\n",
       "            0,     0, 11669,  3339,   500,     0,     0,  7904,  7803,\n",
       "          500,     0,     0, 13453, 12892,   500,     0,     0,  4506,\n",
       "        11605,   500,     0,     0, 12787,  3216,   500,     0,     0,\n",
       "        10880,  4484,   500,     0,     0],\n",
       "       [ 2622, 12870,   612,   240,    98, 12730,  3172,   683,   404,\n",
       "          233,  7154,  6855,   584,   240,     0, 13200,  2703,   942,\n",
       "           84,   152, 13488,  2953,   585,    84,   158,  7733,  7609,\n",
       "          598,   211,   272, 11147,  8560,   711,   515,     0,  3535,\n",
       "        13390,   563,   120,     0, 13697,  3630,   542,    75,   257,\n",
       "        13662,  2831,   523,    75,     0],\n",
       "       [ 2387, 12193,   874,   722,   356,  8997,  6414,   805,   504,\n",
       "          233,  7113,  7291,   853,   815,    73, 12584,  2581,  1239,\n",
       "          334,   152, 12311,  2593,   761,   334,   158,  8141,  8173,\n",
       "          881,   720,   821,  4122,  9425,  1028,   898,     0,  3147,\n",
       "        12979,   872,   691,     0, 13169,  3449,   818,   481,   302,\n",
       "        13693,  3450,   657,   299,     0],\n",
       "       [ 2667, 12393,  1195,  1187,   846,  4448,  9231,  1113,   853,\n",
       "          449,  4823,  4872,  1317,   970,   538, 11829,  1495,  1611,\n",
       "          786,   497, 11037,  1863,   955,   636,   207,  7830,  7634,\n",
       "         1214,  1411,   930,  6726, 12942,  1256,  1068,   215,  3731,\n",
       "        13407,  1172,  1116,   226, 12574,  2255,  1211,   913,   792,\n",
       "        12200,  1344,   852,   732,   187],\n",
       "       [ 3029, 12673,  1520,  1701,  1046,  7673,  1547,  1313,  1183,\n",
       "          859,  2052,  1475,  1642,  1545,   581, 10806,  1444,  1992,\n",
       "         1242,  1064, 12368,  2640,  1195,  1031,   369,  8123,  8060,\n",
       "         1560,  1925,  1177,  5929,  8771,  1594,  1428,   215,  3815,\n",
       "        13866,  1518,  1871,   312, 13570,  3241,  1592,  1328,  1246,\n",
       "        13349,  3729,  1060,  1146,   455],\n",
       "       [ 1642, 11885,  1797,  2153,  1475,  7693,  5231,  1551,  1365,\n",
       "          859,  7666,  7345,  2006,  2180,   581,  7475,  1667,  2262,\n",
       "         1602,  1064,  1233,   527,  1329,  1031,   369,  8283,  8604,\n",
       "         1882,  2397,  1177, 12634,  4910,  1916,  1964,   215,  2508,\n",
       "        12163,  1826,  2471,   312, 13168,  3336,  1778,  1463,  1246,\n",
       "        13643,  2867,  1195,  1222,   455],\n",
       "       [ 1676,  8249,  2338,  2885,  1807,  7281,  7735,  1829,  1721,\n",
       "         1050,  2541,  2345,  2177,  2450,  1506, 10972,  1272,  2625,\n",
       "         2002,  1064, 10754,  1597,  1551,  1370,   369, 13224, 13719,\n",
       "         2163,  2757,  2328, 11029,  3811,  2138,  2151,   215,  3411,\n",
       "        13232,  1969,  2592,  1116, 11695,  1841,  2420,  2008,  1480,\n",
       "        11733,  1718,  1404,  1651,   455],\n",
       "       [ 3248, 11889,  2753,  3609,  2405,  2266,  9179,  2185,  2239,\n",
       "         1050,  6630,  7004,  2678,  2831,  2019, 11606,  1525,  3012,\n",
       "         2534,  1561, 11638,  1842,  1843,  1673,   607,  7869,  7549,\n",
       "         2508,  3331,  2328,  7836, 10011,  2421,  2464,   286,  3935,\n",
       "        13748,  2300,  3197,  1218, 14252, 13081,  2723,  2272,  1738,\n",
       "        13120,  3684,  1619,  1915,   789],\n",
       "       [ 3544, 13177,  3116,  4243,  2730,  4852,  5512,  2397,  2554,\n",
       "         1050,  8019,  5420,  3010,  3364,  2700,  4307,  1438,  3254,\n",
       "         2759,  1759,  4310,  3920,  2020,  1898,   607,  6981,  6553,\n",
       "         3084,  4180,  2914,  7138,  7081,  3068,  3098,   694,  4299,\n",
       "        13715,  2570,  3557,  1218, 12263,  3318,  3103,  2860,  2261,\n",
       "        10779, 10558,  1775,  2163,   789],\n",
       "       [ 3837, 13332,  3841,  5284,  3921,  2662,   740,  2655,  2781,\n",
       "         1981,  7422,  7362,  3217,  3574,  2700, 11885,  3616,  3558,\n",
       "         3285,  1759, 12399,  2135,  2196,  2200,   725,  9522,  9624,\n",
       "         3276,  4391,  3041, 13011,  5219,  3375,  3483,  1312,  3974,\n",
       "        14056,  2855,  4251,  1695, 13424,  3486,  3288,  2953,  2261,\n",
       "        13255,  3770,  1929,  2256,   789],\n",
       "       [10162,  5258,  4261,  5741,  3921, 10270,  5140,  2902,  3140,\n",
       "         2688, 10163,  4972,  3724,  4397,  3729, 10145,  5250,  4477,\n",
       "         3738,  3248,  8676,  5163,  2510,  2656,   834,  9807,  6207,\n",
       "         4101,  5325,  3459, 10108,  4957,  3871,  3741,  2430,  2961,\n",
       "        12953,  3007,  4462,  1695, 10270,  4697,  4065,  3587,  4018,\n",
       "         9951,  6591,  2338,  2919,   968],\n",
       "       [ 7803,  6492,  4594,  6255,  4365,  8675,  5321,  3222,  3554,\n",
       "         2688,  1227, 10985,  4015,  4761,  3871, 10835,  1461,  4676,\n",
       "         3979,  3248,  8597,  5462,  2650,  2656,   960, 11669, 12306,\n",
       "         4434,  5838,  4240,  5217, 10276,  4078,  3911,  2430,  2015,\n",
       "        11891,  3681,  5036,  1695,  9933,  5456,  4187,  3587,  4018,\n",
       "        10038,  6290,  2542,  2919,  1161],\n",
       "       [ 1033,  9167,  4822,  6586,  4365,  3606,  6520,  3575,  4099,\n",
       "         2688,  4174,  7221,  4285,  5302,  3914, 11166,  1512,  5079,\n",
       "         4643,  3248,  9240,  5661,  2851,  2730,  1009,  7074,  8013,\n",
       "         4784,  6258,  4240,  1611, 10866,  4818,  4351,  2430,  1068,\n",
       "        12053,  4482,  5536,  1789, 13482,  4986,  4540,  3951,  4018,\n",
       "        13095,  4438,  2796,  3056,  1245],\n",
       "       [ 3271, 13390,  5602,  7802,  6007,  2712,  8195,  3922,  4528,\n",
       "         2688,  6825, 10906,  4642,  5948,  4229,  8373,  2209,  5388,\n",
       "         5006,  3248,  6011, 10514,  2991,  3210,  1174,  6262, 10244,\n",
       "         5198,  6893,  4911,  6054, 10540,  5187,  4811,  2957,  3421,\n",
       "        13437,  5125,  6516,  3628, 10693,   965,  4939,  4585,  4018,\n",
       "        11038,  1454,  3074,  3079,  1245]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(xs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=30, num_classes=27):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        #out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# class RNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size=64, num_classes=1):\n",
    "#         super(TweetRNN, self).__init__()\n",
    "        \n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         # Forward propagate the RNN\n",
    "#         out, _ = self.rnn(x)\n",
    "#         # Pass the output of the last time step to the classifier\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "\n",
    "\n",
    "# 6*10 input \n",
    "# 64 hidden units \n",
    "# 1 class which is the elo rating \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, xs, ys):\n",
    "       \n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = (self.xs[idx], self.ys[idx])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data):\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=256, shuffle=True)\n",
    "\n",
    "    model.eval() # annotate model for evaluation (why do we need to do this?)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in loader:\n",
    "      \n",
    "#       imgs = imgs.cuda()\n",
    "#       labels = labels.cuda()\n",
    "      z = model(imgs)\n",
    "      pred = z.max(1, keepdim=True)\n",
    "      z = z.detach().numpy()\n",
    "      z = z.argmax(axis=1)\n",
    "      for i in range(len(labels)):\n",
    "        if z[i] == labels[i]:\n",
    "          correct += 1 \n",
    "        total += 1 \n",
    "    print(correct/total)\n",
    "    return correct/total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_data, valid_data, batch_size=32, weight_decay=0.0,learning_rate=0.001, num_epochs=7, max_iters=1000,\n",
    "          checkpoint_path=None, momentum=0.9, save_after=100):\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size)\n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                            lr=learning_rate,\n",
    "                            weight_decay=weight_decay)\n",
    "    # track learning curve\n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "    # training\n",
    "    n = 0 # the number of iterations (for plotting)\n",
    "    for epoch in range(num_epochs):\n",
    "        for imgs, labels in iter(train_loader):\n",
    "#             print(imgs)\n",
    "            model.train() # annotate model for training\n",
    "#             model.cuda()\n",
    "#             imgs = imgs.cuda()\n",
    "#             labels = labels.cuda()\n",
    "            out = model(imgs.float())\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # save the current training information\n",
    "            iters.append(n)\n",
    "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
    "            train_acc.append(get_accuracy(model, train_data)) # compute training accuracy \n",
    "            val_acc.append(get_accuracy(model, valid_data))  # compute validation accuracy\n",
    "\n",
    "            if n % 20 == 0:\n",
    "                if (checkpoint_path is not None) and n > save_after:\n",
    "                    torch.save(model.state_dict(), checkpoint_path.format(n))\n",
    "            if n >= max_iters:\n",
    "                return iters, losses, train_acc, val_acc\n",
    "\n",
    "\n",
    "            n += 1\n",
    "    if (checkpoint_path is not None) and n > save_after:\n",
    "        torch.save(model.state_dict(), checkpoint_path.format(n))\n",
    "    return iters, losses, train_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(50)\n",
    "# model.cuda()\n",
    "# device=torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-3a4db7e58d22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0miters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mmax_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-107-a7fde710ea48>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, train_data, valid_data, batch_size, weight_decay, learning_rate, num_epochs, max_iters, checkpoint_path, momentum, save_after)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0miters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m             \u001b[1;31m# compute *average* loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mtrain_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# compute training accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mval_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# compute validation accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-270f90938bc8>\u001b[0m in \u001b[0;36mget_accuracy\u001b[1;34m(model, data)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#       imgs = imgs.cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#       labels = labels.cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m       \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m       \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m       \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy-ryzen\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-c68484774bc6>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Forward propagate RNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;31m# or:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#out, _ = self.lstm(x, (h0,c0))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy-ryzen\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andy-ryzen\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'RNN_TANH'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m                 result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    472\u001b[0m                                       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m                                       self.batch_first)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "iters, losses, train_acc, val_acc = training(model, train, valid, batch_size=32, num_epochs=50,  max_iters=1000, learning_rate=0.001, momentum=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
